{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "classification_baseline.ipynb",
   "provenance": [],
   "private_outputs": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWGO2uFHpubx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Classification task\n",
    "\n",
    "Hi! It's a classification task baseline notebook.\n",
    "It include a data reader, baseline model and submission generator.\n",
    "\n",
    "You should use GPU to train your model, so we recommend using [Kaggle Notebooks](https://www.kaggle.com/docs/notebooks).\n",
    "To get maximum score of the task, your model should have accuracy greater than `85%`.\n",
    "\n",
    "You can use everything, that suits into the rules in `README.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install catalyst"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Sh__96Cqp1mo"
   },
   "source": [
    "# For Colab user: download dataset and upload zip files.\n",
    "# If you use Kaggle Notebooks, you already have the dataset in a hard drive.\n",
    "\n",
    "# !unzip train.zip -d train\n",
    "# !unzip test.zip -d test"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eHtKdA2dqGEk"
   },
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "\n",
    "import catalyst\n",
    "from catalyst import dl\n",
    "from catalyst.utils import metrics, set_global_seed"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v949FHgsQy62"
   },
   "source": [
    "set_global_seed(42)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgU_DOniQR1p",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "This code will help you to generate dataset. If your data have the following folder structure:\n",
    "\n",
    "```\n",
    "dataset/\n",
    "    class_1/\n",
    "        *.ext\n",
    "        ...\n",
    "    class_2/\n",
    "        *.ext\n",
    "        ...\n",
    "    ...\n",
    "    class_N/\n",
    "        *.ext\n",
    "        ...\n",
    "```\n",
    "First of all `create_dataset` function goes through a given directory and creates a dictionary `Dict[class_name, List[image]]`.\n",
    "Then `create_dataframe` function creates typical `pandas.DataFrame` for further analysis.\n",
    "After than `prepare_dataset_labeling` creates a numerical label for each unique class name.\n",
    "Finally, to add a column with a numerical label value to the DataFrame, we can use `map_dataframe` function.\n",
    "\n",
    "Additionaly let's save the `class_names` for further usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from catalyst.utils import (\n",
    "    create_dataset, create_dataframe, get_dataset_labeling, map_dataframe\n",
    ")\n",
    "\n",
    "dataset = create_dataset(dirs=f\"train/*\", extension=\"*.jpg\")\n",
    "df = create_dataframe(dataset, columns=[\"class\", \"filepath\"])\n",
    "\n",
    "tag_to_label = get_dataset_labeling(df, \"class\")\n",
    "class_names = [\n",
    "    name for name, id_ in sorted(tag_to_label.items(), key=lambda x: x[1])\n",
    "]\n",
    "\n",
    "df_with_labels = map_dataframe(\n",
    "    df, \n",
    "    tag_column=\"class\", \n",
    "    class_column=\"label\", \n",
    "    tag2class=tag_to_label, \n",
    "    verbose=False\n",
    ")\n",
    "df_with_labels.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And you should split data in `train/valid/test` parts.\n",
    "There are only `train` and `valid` parts, so you must load test data as shows in a code cell."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sEhBS2sqQZ1z"
   },
   "source": [
    "from catalyst.utils import split_dataframe_train_test\n",
    "\n",
    "train_data, valid_data = split_dataframe_train_test(\n",
    "    df_with_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "train_data, valid_data = (\n",
    "    train_data.to_dict(\"records\"),\n",
    "    valid_data.to_dict(\"records\"),\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-1upnlv4RV2m"
   },
   "source": [
    "from catalyst.data.cv.reader import ImageReader\n",
    "from catalyst.dl import utils\n",
    "from catalyst.data import ScalarReader, ReaderCompose\n",
    "\n",
    "num_classes = len(tag_to_label)\n",
    "\n",
    "open_fn = ReaderCompose(\n",
    "    [\n",
    "        ImageReader(\n",
    "            input_key=\"filepath\", output_key=\"features\", rootpath=\"train\"\n",
    "        ),\n",
    "        ScalarReader(\n",
    "            input_key=\"label\",\n",
    "            output_key=\"targets\",\n",
    "            default_value=-1,\n",
    "            dtype=np.int64,\n",
    "        ),\n",
    "        ScalarReader(\n",
    "            input_key=\"label\",\n",
    "            output_key=\"targets_one_hot\",\n",
    "            default_value=-1,\n",
    "            dtype=np.int64,\n",
    "            one_hot_classes=num_classes,\n",
    "        ),\n",
    "    ]\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Augmentation\n",
    "\n",
    "In a baseline, we don't have augmentation transformations in the baseline.\n",
    "You can add them, if you expect that it will increase model accuracy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G1f7StMdRV0S"
   },
   "source": [
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensor\n",
    "\n",
    "BORDER_CONSTANT = 0\n",
    "BORDER_REFLECT = 2\n",
    "\n",
    "transforms = albu.Compose(\n",
    "    [\n",
    "        albu.Resize(224, 224, p=1),\n",
    "        albu.Normalize(),\n",
    "        ToTensor(),\n",
    "    ]\n",
    ")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wJ1hwc8NRVxB"
   },
   "source": [
    "from catalyst.data import Augmentor\n",
    "\n",
    "data_transforms = Augmentor(\n",
    "    dict_key=\"features\", augment_fn=lambda x: transforms(image=x)[\"image\"]\n",
    ")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-4HVfMiRuxM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Don't forget creating test loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = utils.get_loader(\n",
    "    train_data,\n",
    "    open_fn=open_fn,\n",
    "    dict_transform=data_transforms,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=True,\n",
    "    sampler=None,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "valid_loader = utils.get_loader(\n",
    "    valid_data,\n",
    "    open_fn=open_fn,\n",
    "    dict_transform=data_transforms,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False, \n",
    "    sampler=None,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "loaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"valid\": valid_loader\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "\n",
    "For the baseline, we will use a ResNet model.\n",
    "We already have examined in the seminar.\n",
    "Enhance the model, use any* instruments or module as you like.\n",
    "\n",
    "*(Don't forget about the rules!)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OpOxdntTrAwP"
   },
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, p=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=stride,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                out_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "        self.res = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=1, stride=stride\n",
    "        )\n",
    "        self.output = nn.Sequential(nn.BatchNorm2d(out_channels), nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = self.input(x)\n",
    "        res = self.res(x)\n",
    "        return self.output(res + input)\n",
    "\n",
    "\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, channels=3, in_features=64, num_classes=10, p=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                channels, in_features, kernel_size=7, stride=2, padding=3\n",
    "            ),\n",
    "            nn.BatchNorm2d(in_features),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "\n",
    "        self.layer_0 = self._make_layer(in_features, 1)\n",
    "        self.layer_1 = self._make_layer(in_features)\n",
    "        in_features *= 2\n",
    "        self.layer_2 = self._make_layer(in_features)\n",
    "        in_features *= 2\n",
    "        self.layer_3 = self._make_layer(in_features)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2 * in_features, num_classes),\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, in_features, multiplier=2, p=0.1):\n",
    "        return nn.Sequential(\n",
    "            ResNetBlock(in_features, in_features * multiplier, stride=2, p=p),\n",
    "            ResNetBlock(\n",
    "                in_features * multiplier,\n",
    "                in_features * multiplier,\n",
    "                stride=1,\n",
    "                p=p,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        x = self.layer_0(x)\n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        return self.fc(x)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B26FN-CLq4cF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Create Runner and train your model!\n",
    "\n",
    "class Runner(dl.Runner):\n",
    "    def predict_batch(self, batch):\n",
    "        return self.model(batch[0].to(self.device))\n",
    "\n",
    "    def _handle_batch(self, batch):\n",
    "        y_pred = self.model(batch[\"features\"])\n",
    "\n",
    "        self.input = batch\n",
    "        self.output = {\"logits\": y_pred}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HOfvcgRNrNWh"
   },
   "source": [
    "model = BaselineModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "runner = Runner()\n",
    "runner.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    loaders=loaders,\n",
    "    logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    "    load_best_on_end=True,\n",
    "    callbacks={\n",
    "        \"optimizer\": dl.OptimizerCallback(\n",
    "            metric_key=\"loss\", accumulation_steps=1, grad_clip_params=None,\n",
    "        ),\n",
    "        \"criterion\": dl.CriterionCallback(\n",
    "            input_key=\"targets\", output_key=\"logits\", prefix=\"loss\",\n",
    "        ),\n",
    "        \"accuracy\": dl.AccuracyCallback(num_classes=10),\n",
    "    },\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This code below will generate a submission.\n",
    "It reads images from `test` folder and gathers prediction from the trained model.\n",
    "Check your submission before uploading it into `Kaggle`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KgNpIL-uS5KB"
   },
   "source": [
    "from PIL import Image\n",
    "\n",
    "submission = {\"Id\": [], \"Category\": []}\n",
    "model.eval()\n",
    "\n",
    "for file in Path(\"test\").iterdir():\n",
    "    image = np.array(Image.open(file).convert('RGB'))\n",
    "    features = transforms(image=image)[\"image\"].unsqueeze(0).cuda()\n",
    "    pred = model(features)\n",
    "    pred = pred.detach().cpu().numpy()\n",
    "    pred = np.argmax(pred)\n",
    "    submission[\"Id\"].append(file.name[:-4])\n",
    "    submission[\"Category\"].append(class_names[pred])\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "k8XOQD15S5Gh"
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.DataFrame(submission).to_csv(\"baseline.csv\", index=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OF2oj8uFd2-J",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "pd.DataFrame(submission)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}